{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from cube import Cube\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from value_policy_net import ValuePolicyNet\n",
    "from resnet_model import ResnetModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "l = 100\n",
    "M = 20\n",
    "\n",
    "lr = 1e-4\n",
    "batch_len = 32\n",
    "epochs_per_iteration = 1\n",
    "reward_type = 'deepcube' # what reward type to use, makes a difference only if value_only = False (DeepCube / MCTS). Values are 'deepcube' or 'dqn'.\n",
    "network_type = 'resnet' # what network type to use, makes a difference only if value_only = True (DeepCubeA / A*). Values are 'normal' or 'resnet'.\n",
    "value_only = True # value_only False trains the model for DeepCube (mcts), while True trains the model for DeepCubeA (A*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encode = {\n",
    "    'F': 0,\n",
    "    'F\\'': 1,\n",
    "    'B': 2,\n",
    "    'B\\'': 3,\n",
    "    'L': 4,\n",
    "    'L\\'': 5,\n",
    "    'R': 6,\n",
    "    'R\\'': 7,\n",
    "    'U': 8,\n",
    "    'U\\'': 9,\n",
    "    'D': 10,\n",
    "    'D\\'': 11,\n",
    "}\n",
    "action_decode = {encoding: action for action,\n",
    "                 encoding in action_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_net = ValuePolicyNet(value_only = value_only).to(device) if network_type == 'normal' else ResnetModel(20, 24, 5000, 1000, 4, 1, True).to(device)\n",
    "optim = torch.optim.RMSprop(vp_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scrambled_cubes(k, l):\n",
    "    states = []\n",
    "    for _ in range(l):\n",
    "        cube = Cube()\n",
    "        scramble_str = cube.get_scramble(k)\n",
    "        for rot_num, rot_code in enumerate(scramble_str.split(' ')):\n",
    "            cube.rotate_code(rot_code)\n",
    "            states.append(\n",
    "                (np.copy(cube.facelets), np.copy(cube.tracked), rot_num + 1))\n",
    "\n",
    "    return states\n",
    "\n",
    "\n",
    "def batchify(X, Y, W, batch_size):\n",
    "    x_batch = []\n",
    "    y_v_batch = []\n",
    "    y_p_batch = []\n",
    "    w_batch = []\n",
    "    for x, (y_v, y_p), w in zip(X, Y, W):\n",
    "        x_batch.append(x)\n",
    "        y_v_batch.append(y_v)\n",
    "        y_p_batch.append(y_p)\n",
    "        w_batch.append(w)\n",
    "        if len(x_batch) >= batch_size:\n",
    "            if value_only:\n",
    "                yield torch.Tensor(np.array(x_batch)).to(device), torch.Tensor(y_v_batch).to(device), None, torch.Tensor(w_batch).to(device)\n",
    "            else:\n",
    "                yield torch.Tensor(np.array(x_batch)).to(device), torch.Tensor(y_v_batch).to(device), torch.Tensor(y_p_batch).to(device), torch.Tensor(w_batch).to(device)\n",
    "            x_batch.clear()\n",
    "            y_v_batch.clear()\n",
    "            y_p_batch.clear()\n",
    "            w_batch.clear()\n",
    "    if len(x_batch) > 0:\n",
    "        if value_only:\n",
    "            yield torch.Tensor(np.array(x_batch)).to(device), torch.Tensor(y_v_batch).to(device), None, torch.Tensor(w_batch).to(device)\n",
    "        else:\n",
    "            yield torch.Tensor(np.array(x_batch)).to(device), torch.Tensor(y_v_batch).to(device), torch.Tensor(y_p_batch).to(device), torch.Tensor(w_batch).to(device)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for it in range(M):\n",
    "    cube = Cube()\n",
    "    scrambled_cubes = get_scrambled_cubes(k, l)\n",
    "    F = [f for f, _, _ in scrambled_cubes]\n",
    "    T = [t for _, t, _ in scrambled_cubes]\n",
    "    W = [1 / n for _, _, n in scrambled_cubes]\n",
    "    X = [Cube.encode_state(t, cube.edges_corners) for t in T]\n",
    "    Y = []\n",
    "\n",
    "    vp_net.eval()\n",
    "    for f, t in zip(F, T):\n",
    "        v_x = []\n",
    "        r_x = []\n",
    "        solved = []\n",
    "        for a in range(12):\n",
    "            cube.facelets = np.copy(f)\n",
    "            cube.tracked = np.copy(t)\n",
    "            r_x_a = cube.rotate_code_get_reward(action_decode[a], reward_type)\n",
    "            with torch.no_grad():\n",
    "                net_out = vp_net(torch.Tensor(Cube.encode_state(cube.tracked, cube.edges_corners))[None, :].to(device))\n",
    "                # Even in the case of both value and policy, we only care about the value of the next state\n",
    "                v_x_a = net_out[0] if value_only else net_out[0][0]\n",
    "            solved.append(cube.is_solved())\n",
    "\n",
    "            v_x.append(v_x_a)\n",
    "            r_x.append(r_x_a)\n",
    "\n",
    "        v_x = torch.Tensor(v_x)\n",
    "        if not value_only: # For value and policy we set v_next to the reward if solved\n",
    "            r_x = torch.Tensor(r_x)\n",
    "\n",
    "            v_next = r_x + v_x\n",
    "            for a in range(12):\n",
    "                if solved[a]:\n",
    "                    v_next[a] = r_x[a]\n",
    "        else: # For value only, we don't care about rewards, as value is actually the approximated number of steps to solve\n",
    "            v_next = 1 + v_x\n",
    "            \n",
    "            for a in range(12):\n",
    "                if solved[a]:\n",
    "                    v_next[a] = 1 # TODO: makes more sense to be 1 here but what if it's actually 0? Maybe skim the paper again to check\n",
    "                \n",
    "        if not value_only: # For value and policy we get the max of the next values\n",
    "            y_v = torch.max(v_next)\n",
    "            y_p = torch.argmax(v_next)\n",
    "            Y.append((y_v, y_p))\n",
    "        else: # For value only we get the min of the next values\n",
    "            y_v = torch.min(v_next)\n",
    "            Y.append((y_v, None))\n",
    "    vp_net.train()\n",
    "\n",
    "    shuffle_indices = np.arange(len(X))\n",
    "    # np.random.shuffle(shuffle_indices)\n",
    "    X = [X[i] for i in shuffle_indices]\n",
    "    Y = [Y[i] for i in shuffle_indices]\n",
    "    W = [W[i] for i in shuffle_indices]\n",
    "    \n",
    "    it_losses = []\n",
    "    for epoch in range(epochs_per_iteration):\n",
    "        epoch_losses = []\n",
    "        for x_batch, y_v_batch, y_p_batch, w_batch in batchify(X, Y, W, batch_len):\n",
    "            optim.zero_grad()\n",
    "            if not value_only: # For value and policy we use both the loss of value and policy\n",
    "                y_v_pred, y_p_pred = vp_net(x_batch)\n",
    "                v_loss = torch.mean(w_batch * (y_v_batch - y_v_pred) ** 2)\n",
    "                p_loss = torch.mean(\n",
    "                    w_batch * torch.nn.CrossEntropyLoss(reduction='none')(y_p_pred, y_p_batch.long()))\n",
    "                loss = v_loss + p_loss\n",
    "            else: # For value only we use only the loss of value\n",
    "                y_v_pred = vp_net(x_batch)\n",
    "                v_loss = torch.mean(w_batch * (y_v_batch - y_v_pred) ** 2)\n",
    "                loss = v_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "        it_losses.append(np.mean(epoch_losses))\n",
    "    it_loss = np.mean(it_losses)\n",
    "    losses.append(it_loss)\n",
    "\n",
    "    print(f'Iteration {it}: loss = {it_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title('Average loss per iteration')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vp_net.state_dict(), 'vp_net.pt' if not value_only else f'v_net_{network_type}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
